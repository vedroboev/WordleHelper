{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "from tqdm import tqdm as ProgressDisplay\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISS = np.uint8(0)\n",
    "MISPLACED = np.uint8(1)\n",
    "EXACT = np.uint8(2)\n",
    "\n",
    "# DATA_DIR = os.path.join(\n",
    "#     os.path.dirname(os.path.realpath(__file__)),\n",
    "#     \"data\",\n",
    "# )\n",
    "DATA_DIR = \".\"\n",
    "\n",
    "SHORT_WORD_LIST_FILE = os.path.join(DATA_DIR, \"words_test.txt\")\n",
    "LONG_WORD_LIST_FILE = os.path.join(DATA_DIR, \"words_test.txt\")\n",
    "# WORD_FREQ_FILE = os.path.join(DATA_DIR, \"wordle_words_freqs_full.txt\")\n",
    "# WORD_FREQ_MAP_FILE = os.path.join(DATA_DIR, \"freq_map.json\")\n",
    "# SECOND_GUESS_MAP_FILE = os.path.join(DATA_DIR, \"second_guess_map.json\")\n",
    "PATTERN_MATRIX_FILE = os.path.join(DATA_DIR, \"pattern_matrix.npy\")\n",
    "# ENT_SCORE_PAIRS_FILE = os.path.join(DATA_DIR, \"ent_score_pairs.json\")\n",
    "\n",
    "# To store the large grid of patterns at run time\n",
    "PATTERN_GRID_DATA = dict()\n",
    "\n",
    "\n",
    "def safe_log2(x):\n",
    "    return math.log2(x) if x > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(short=False):\n",
    "    result = []\n",
    "    file = SHORT_WORD_LIST_FILE if short else LONG_WORD_LIST_FILE\n",
    "    with open(file) as fp:\n",
    "        result.extend([word.strip() for word in fp.readlines()])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_int_arrays(words):\n",
    "    return np.array([[ord(c)for c in w] for w in words], dtype=np.uint8)\n",
    "\n",
    "\n",
    "def generate_pattern_matrix(words1, words2):\n",
    "    \"\"\"\n",
    "    A pattern for two words represents the wordle-similarity\n",
    "    pattern (grey -> 0, yellow -> 1, green -> 2) but as an integer\n",
    "    between 0 and 3^5. Reading this integer in ternary gives the\n",
    "    associated pattern.\n",
    "\n",
    "    This function computes the pairwise patterns between two lists\n",
    "    of words, returning the result as a grid of hash values. Since\n",
    "    this can be time-consuming, many operations that can be are vectorized\n",
    "    (perhaps at the expense of easier readibility), and the the result\n",
    "    is saved to file so that this only needs to be evaluated once, and\n",
    "    all remaining pattern matching is a lookup.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of letters/words\n",
    "    nl = len(words1[0])\n",
    "    nw1 = len(words1)  # Number of words\n",
    "    nw2 = len(words2)  # Number of words\n",
    "\n",
    "    # Convert word lists to integer arrays\n",
    "    word_arr1, word_arr2 = map(words_to_int_arrays, (words1, words2))\n",
    "\n",
    "    # equality_grid keeps track of all equalities between all pairs\n",
    "    # of letters in words. Specifically, equality_grid[a, b, i, j]\n",
    "    # is true when words[i][a] == words[b][j]\n",
    "    equality_grid = np.zeros((nw1, nw2, nl, nl), dtype=bool)\n",
    "    for i, j in it.product(range(nl), range(nl)):\n",
    "        equality_grid[:, :, i, j] = np.equal.outer(word_arr1[:, i], word_arr2[:, j])\n",
    "\n",
    "    # full_pattern_matrix[a, b] should represent the 5-color pattern\n",
    "    # for guess a and answer b, with 0 -> grey, 1 -> yellow, 2 -> green\n",
    "    full_pattern_matrix = np.zeros((nw1, nw2, nl), dtype=np.uint8)\n",
    "\n",
    "    # Green pass\n",
    "    for i in range(nl):\n",
    "        matches = equality_grid[:, :, i, i].flatten()  # matches[a, b] is true when words[a][i] = words[b][i]\n",
    "        full_pattern_matrix[:, :, i].flat[matches] = EXACT\n",
    "\n",
    "        for k in range(nl):\n",
    "            # If it's a match, mark all elements associated with\n",
    "            # that letter, both from the guess and answer, as covered.\n",
    "            # That way, it won't trigger the yellow pass.\n",
    "            equality_grid[:, :, k, i].flat[matches] = False\n",
    "            equality_grid[:, :, i, k].flat[matches] = False\n",
    "\n",
    "    # Yellow pass\n",
    "    for i, j in it.product(range(nl), range(nl)):\n",
    "        matches = equality_grid[:, :, i, j].flatten()\n",
    "        full_pattern_matrix[:, :, i].flat[matches] = MISPLACED\n",
    "        for k in range(nl):\n",
    "            # Similar to above, we want to mark this letter\n",
    "            # as taken care of, both for answer and guess\n",
    "            equality_grid[:, :, k, j].flat[matches] = False\n",
    "            equality_grid[:, :, i, k].flat[matches] = False\n",
    "\n",
    "    # Rather than representing a color pattern as a lists of integers,\n",
    "    # store it as a single integer, whose ternary representations corresponds\n",
    "    # to that list of integers.\n",
    "    pattern_matrix = np.dot(\n",
    "        full_pattern_matrix,\n",
    "        (3**np.arange(nl)).astype(np.uint8)\n",
    "    )\n",
    "\n",
    "    return pattern_matrix\n",
    "\n",
    "\n",
    "def generate_full_pattern_matrix():\n",
    "    words = get_word_list()\n",
    "    pattern_matrix = generate_pattern_matrix(words, words)\n",
    "    # Save to file\n",
    "    np.save(PATTERN_MATRIX_FILE, pattern_matrix)\n",
    "    return pattern_matrix\n",
    "\n",
    "\n",
    "def get_pattern_matrix(words1, words2):\n",
    "    if not PATTERN_GRID_DATA:\n",
    "        if not os.path.exists(PATTERN_MATRIX_FILE):\n",
    "            print(\"\\n\".join([\n",
    "                \"Generating pattern matrix. This takes a minute, but\",\n",
    "                \"the result will be saved to file so that it only\",\n",
    "                \"needs to be computed once.\",\n",
    "            ]))\n",
    "            generate_full_pattern_matrix()\n",
    "        PATTERN_GRID_DATA['grid'] = np.load(PATTERN_MATRIX_FILE)\n",
    "        PATTERN_GRID_DATA['words_to_index'] = dict(zip(\n",
    "            get_word_list(), it.count()\n",
    "        ))\n",
    "\n",
    "    full_grid = PATTERN_GRID_DATA['grid']\n",
    "    words_to_index = PATTERN_GRID_DATA['words_to_index']\n",
    "\n",
    "    indices1 = [words_to_index[w] for w in words1]\n",
    "    indices2 = [words_to_index[w] for w in words2]\n",
    "    return full_grid[np.ix_(indices1, indices2)]\n",
    "\n",
    "\n",
    "def get_pattern(guess, answer):\n",
    "    if PATTERN_GRID_DATA:\n",
    "        saved_words = PATTERN_GRID_DATA['words_to_index']\n",
    "        if guess in saved_words and answer in saved_words:\n",
    "            return get_pattern_matrix([guess], [answer])[0, 0]\n",
    "    return generate_pattern_matrix([guess], [answer])[0, 0]\n",
    "\n",
    "\n",
    "def pattern_from_string(pattern_string):\n",
    "    return sum((3**i) * int(c) for i, c in enumerate(pattern_string))\n",
    "\n",
    "\n",
    "def pattern_to_int_list(pattern):\n",
    "    result = []\n",
    "    curr = pattern\n",
    "    for x in range(5):\n",
    "        result.append(curr % 3)\n",
    "        curr = curr // 3\n",
    "    return result\n",
    "\n",
    "\n",
    "def pattern_to_string(pattern):\n",
    "    d = {MISS: \"拘뜦", MISPLACED: \"游릳\", EXACT: \"游릴\"}\n",
    "    return \"\".join(d[x] for x in pattern_to_int_list(pattern))\n",
    "\n",
    "\n",
    "def patterns_to_string(patterns):\n",
    "    return \"\\n\".join(map(pattern_to_string, patterns))\n",
    "\n",
    "\n",
    "def get_possible_words(guess, pattern, word_list):\n",
    "    all_patterns = get_pattern_matrix([guess], word_list).flatten()\n",
    "    return list(np.array(word_list)[all_patterns == pattern])\n",
    "\n",
    "\n",
    "def get_word_buckets(guess, possible_words):\n",
    "    buckets = [[] for x in range(3**5)]\n",
    "    hashes = get_pattern_matrix([guess], possible_words).flatten()\n",
    "    for index, word in zip(hashes, possible_words):\n",
    "        buckets[index].append(word)\n",
    "    return buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(words, priors):\n",
    "    frequencies = np.array([priors[word] for word in words])\n",
    "    total = frequencies.sum()\n",
    "    if total == 0:\n",
    "        return np.zeros(frequencies.shape)\n",
    "    return frequencies / total\n",
    "\n",
    "\n",
    "def get_pattern_distributions(allowed_words, possible_words, weights):\n",
    "    \"\"\"\n",
    "    For each possible guess in allowed_words, this finds the probability\n",
    "    distribution across all of the 3^5 wordle patterns you could see, assuming\n",
    "    the possible answers are in possible_words with associated probabilities\n",
    "    in weights.\n",
    "\n",
    "    It considers the pattern hash grid between the two lists of words, and uses\n",
    "    that to bucket together words from possible_words which would produce\n",
    "    the same pattern, adding together their corresponding probabilities.\n",
    "    \"\"\"\n",
    "    pattern_matrix = get_pattern_matrix(allowed_words, possible_words)\n",
    "\n",
    "    n = len(allowed_words)\n",
    "    distributions = np.zeros((n, 3**5))\n",
    "    n_range = np.arange(n)\n",
    "    for j, prob in enumerate(weights):\n",
    "        distributions[n_range, pattern_matrix[:, j]] += prob\n",
    "    return distributions\n",
    "\n",
    "\n",
    "def entropy_of_distributions(distributions, atol=1e-12):\n",
    "    axis = len(distributions.shape) - 1\n",
    "    return entropy(distributions, base=2, axis=axis)\n",
    "\n",
    "\n",
    "def get_entropies(allowed_words, possible_words, weights):\n",
    "    if weights.sum() == 0:\n",
    "        return np.zeros(len(allowed_words))\n",
    "    distributions = get_pattern_distributions(allowed_words, possible_words, weights)\n",
    "    return entropy_of_distributions(distributions)\n",
    "\n",
    "\n",
    "def max_bucket_size(guess, possible_words, weights):\n",
    "    dist = get_pattern_distributions([guess], possible_words, weights)\n",
    "    return dist.max()\n",
    "\n",
    "\n",
    "def words_to_max_buckets(possible_words, weights):\n",
    "    return dict(\n",
    "        (word, max_bucket_size(word, possible_words, weights))\n",
    "        for word in ProgressDisplay(possible_words)\n",
    "    )\n",
    "\n",
    "    # words_and_maxes = list(w2m.items())\n",
    "    # words_and_maxes.sort(key=lambda t: t[1])\n",
    "    # words_and_maxes[:-20:-1]\n",
    "\n",
    "\n",
    "def get_bucket_sizes(allowed_words, possible_words):\n",
    "    \"\"\"\n",
    "    Returns a (len(allowed_words), 243) shape array reprenting the size of\n",
    "    word buckets associated with each guess in allowed_words\n",
    "    \"\"\"\n",
    "    weights = np.ones(len(possible_words))\n",
    "    return get_pattern_distributions(allowed_words, possible_words, weights)\n",
    "\n",
    "\n",
    "def get_bucket_counts(allowed_words, possible_words):\n",
    "    \"\"\"\n",
    "    Returns the number of separate buckets that each guess in allowed_words\n",
    "    would separate possible_words into\n",
    "    \"\"\"\n",
    "    bucket_sizes = get_bucket_sizes(allowed_words, possible_words)\n",
    "    return (bucket_sizes > 0).sum(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_second_step_entropies(first_guesses, allowed_second_guesses, possible_words, priors):\n",
    "    result = []\n",
    "    weights = get_weights(possible_words, priors)\n",
    "    if weights.sum() == 0:\n",
    "        return np.zeros(len(first_guesses))\n",
    "\n",
    "    distributions = get_pattern_distributions(first_guesses, possible_words, weights)\n",
    "    for first_guess, dist in ProgressDisplay(list(zip(first_guesses, distributions)), leave=False, desc=\"Searching 2nd step entropies\"):\n",
    "        word_buckets = get_word_buckets(first_guess, possible_words)\n",
    "        # List of maximum entropies you could achieve in\n",
    "        # the second step for each pattern you might see\n",
    "        # after this setp\n",
    "        ents2 = np.array([\n",
    "            get_entropies(\n",
    "                allowed_words=allowed_second_guesses,\n",
    "                possible_words=bucket,\n",
    "                weights=get_weights(bucket, priors)\n",
    "            ).max()\n",
    "            for bucket in word_buckets\n",
    "        ])\n",
    "        # Multiply each such maximal entropy by the corresponding\n",
    "        # probability of falling into that bucket\n",
    "        result.append(np.dot(ents2, dist))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[242,   2,   2,   0,   9,   0,   0,   0,  10,   0,   0,   0,  10,\n",
       "          1,  63,   1,   0,   0,   1,   0,   1,  10,   9,  10, 180],\n",
       "       [  2, 242,  83,   0,  24,   6,  12, 171,   4, 195, 195,  36,   7,\n",
       "          4,   4,  10,  54,  90,   1, 180,  82,  82,  12,   1,   3],\n",
       "       [  2,  11, 242,   0,   0,   0,   3,   9,   1,   9,  12,  27,   4,\n",
       "          7,   1,   1, 162,  21, 190,  36,  37,  37,   0,   4,   0],\n",
       "       [  0,   0,   0, 242,  11,   2,   2,   2,  54,   3,   3,   1,   0,\n",
       "         54,   0,   0,   0,   0,  18,   0,  54,   0,   9,   0,   6],\n",
       "       [ 81,  24,   0,  29, 242,   8,  14,  11,  84,   6,   6,  10, 168,\n",
       "          3,  84,   9,   0,   9,  54,  18,   0, 162, 120,  81,  84],\n",
       "       [  0,   6,   0,   2,   8, 242,   5,  74,   3,  60,   6,  10,  87,\n",
       "         84,  84,   0,   9,   0,   0,  36,  27,   0,   3,  81,   3],\n",
       "       [  0,  12,  81,   2,  14,  11, 242,   8,   9,   9,  90,   7,  90,\n",
       "         90,   9,   3,   0, 222,   0,   3,   0,   0,  15, 162,   9],\n",
       "       [  0, 165,  81,   2,   5,  74,   8, 242,   0, 216, 162,  16,   0,\n",
       "          0,   0,   3,   9,  87,   0, 201, 108,  81,   6,   0,   0],\n",
       "       [ 10,  90,   9,  54,  82,  81,  81,   0, 242, 108, 108,   0, 100,\n",
       "        234, 172,  12,   0,   0,   9,   0,  72,  10, 163,  10, 109],\n",
       "       [  0, 169,  81,   9,   6,  60,   3, 216,  12, 242, 188,   2,   6,\n",
       "         12,   3,   0,   1,  81,   0, 189, 117,  81,   3,   0,  12],\n",
       "       [  0, 169, 108,   9,   6,   6,  30, 162,  12, 188, 242,   2,  60,\n",
       "         39,   3,   0,   1, 108,   0, 162,  90,  81,   3,  27,  12],\n",
       "       [  0,   4,  81,   9,  12,  36,  15,  42,   0,   2,   2, 242,   0,\n",
       "          0,   0,   3,  28,   6,  81, 138,  81,  81,   6,   0,   0],\n",
       "       [ 90,  15,  36,   0, 168,   7,  30,   0, 102,   6,  60,   0, 242,\n",
       "         50,  94,   9,   0,  27,   9,   0,  18, 171,  84, 118,  84],\n",
       "       [  9,  90,  15,  54,  81,  82,  84,   0, 234, 108, 111,   0, 104,\n",
       "        242, 172,   9,   0,   3,   9,   0,  72,   9, 162,  13, 108],\n",
       "       [ 57,  28,  27,   0,   4,  10,   1,   0, 192,   1,   1,   0,  40,\n",
       "        198, 242,  27,   0,   0,  27,   0,  27,  30, 165,  48,   4],\n",
       "       [  3,  84,   3,   0,  81,   0,  81,  81,   4,   0,   0,  81,   3,\n",
       "          3,   3, 242,   0,  81,   6,  81,   3,   6,  81,   6,   0],\n",
       "       [  0,  54, 162,   0,   0,   3,   0,   3,   0,   9,   9,  12,   0,\n",
       "          0,   0,   0, 242,   0, 162,   3,   0,   0,   0,   0,   0],\n",
       "       [  0,  12,  19,   0,   3,   0, 222,  15,   0,   9,  10,   6,   1,\n",
       "          1,   0,   3,   0, 242,   0,  12,   9,   9,   6, 162,   0],\n",
       "       [  3,   3, 166,  18,  54,   0,   0,   0,   3,   0,   0,   1,   3,\n",
       "          3,   3,   6, 162,   0, 242,   2,   5,   7,   9,   6,   0],\n",
       "       [  0, 180,  82,   0,  18,  30,   9, 201,   0, 165, 162,  64,   0,\n",
       "          0,   0,   9,  27,  90,   2, 242,  86,  82,   9,   0,   0],\n",
       "       [  9,  12,  13,  54,   0,  81,   0,  84,  72, 111,  30,   1,  18,\n",
       "         72,   9,   9,   0,   3,  11,  86, 242,  13,   0,   9,  27],\n",
       "       [ 84,   4,  13,   0, 162,   0,   0,   1,  84,   1,   1,   9, 165,\n",
       "          3,  84,   6,   0,   1,  15,  10,  13, 242,  81,  87,  81],\n",
       "       [ 27,  84,   0,   1, 112,  81,  87,   6, 189,  81,  81,   6, 108,\n",
       "        162, 189,   3,   0,   6,   1,   3,   0,  27, 242,  63, 117],\n",
       "       [ 30,   3,  84,   0,  27,   9, 162,   0,  30,   0,  81,   0, 120,\n",
       "         93,  48,   6,   0, 162,   6,   0,   3,  33,  55, 242,  29],\n",
       "       [180,  27,   0,   6,  36,  27,  27,   0,  39,  30,  30,   0,  36,\n",
       "         30,  36,   0,   0,   0,   0,   0,   3,   9,  37,  11, 242]],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_full_pattern_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'游릴游릴游릴游릴游릴'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_to_string(242)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
